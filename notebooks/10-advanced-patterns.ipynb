{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 10: Advanced Patterns\n",
    "\n",
    "Master production-ready patterns and best practices for building robust, scalable LangGraph applications. Learn error handling, monitoring, optimization, and advanced architectural patterns.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement error handling and recovery\n",
    "- Add monitoring and logging\n",
    "- Optimize graph performance\n",
    "- Use advanced architectural patterns\n",
    "- Build production-ready applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import getpass\n",
    "from typing import TypedDict, Literal, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "if \"ANTHROPIC_API_KEY\" not in os.environ:\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-sonnet-4-20250514\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Error Handling and Recovery\n",
    "\n",
    "Robust applications must handle errors gracefully and recover when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustState(TypedDict):\n",
    "    input: str\n",
    "    result: str\n",
    "    error: Optional[str]\n",
    "    retry_count: int\n",
    "    max_retries: int\n",
    "\n",
    "def risky_operation(state: RobustState) -> RobustState:\n",
    "    \"\"\"Operation that might fail - with error handling.\"\"\"\n",
    "    print(f\"\\nðŸ”„ Attempt {state['retry_count'] + 1}/{state['max_retries']}\")\n",
    "    \n",
    "    try:\n",
    "        # Simulate potential failure\n",
    "        if state['retry_count'] < 2:  # Fail first 2 times for demo\n",
    "            raise Exception(\"Simulated API error\")\n",
    "        \n",
    "        # Actual operation\n",
    "        response = model.invoke([HumanMessage(content=state['input'])])\n",
    "        \n",
    "        logger.info(\"Operation succeeded\")\n",
    "        return {\n",
    "            \"result\": response.content,\n",
    "            \"error\": None,\n",
    "            \"retry_count\": state['retry_count'] + 1\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Operation failed: {str(e)}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"retry_count\": state['retry_count'] + 1\n",
    "        }\n",
    "\n",
    "def error_handler(state: RobustState) -> RobustState:\n",
    "    \"\"\"Handle errors with fallback logic.\"\"\"\n",
    "    print(\"\\nâš ï¸  Error handler activated\")\n",
    "    logger.warning(f\"Handling error: {state['error']}\")\n",
    "    \n",
    "    return {\n",
    "        \"result\": f\"Operation failed after {state['retry_count']} attempts. Error: {state['error']}\"\n",
    "    }\n",
    "\n",
    "def should_retry(state: RobustState) -> Literal[\"retry\", \"error\", \"success\"]:\n",
    "    \"\"\"Decide whether to retry, handle error, or succeed.\"\"\"\n",
    "    if state['error'] is None:\n",
    "        print(\"âœ… Success!\")\n",
    "        return \"success\"\n",
    "    elif state['retry_count'] < state['max_retries']:\n",
    "        print(\"ðŸ”„ Retrying...\")\n",
    "        time.sleep(1)  # Backoff\n",
    "        return \"retry\"\n",
    "    else:\n",
    "        print(\"âŒ Max retries reached\")\n",
    "        return \"error\"\n",
    "\n",
    "print(\"âœ“ Error handling nodes created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build robust graph with error handling\n",
    "robust_builder = StateGraph(RobustState)\n",
    "\n",
    "robust_builder.add_node(\"operation\", risky_operation)\n",
    "robust_builder.add_node(\"error_handler\", error_handler)\n",
    "\n",
    "robust_builder.add_edge(START, \"operation\")\n",
    "\n",
    "robust_builder.add_conditional_edges(\n",
    "    \"operation\",\n",
    "    should_retry,\n",
    "    {\n",
    "        \"retry\": \"operation\",  # Loop back for retry\n",
    "        \"error\": \"error_handler\",\n",
    "        \"success\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "robust_builder.add_edge(\"error_handler\", END)\n",
    "\n",
    "robust_graph = robust_builder.compile()\n",
    "\n",
    "print(\"âœ“ Robust graph compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling and retry logic\n",
    "print(\"Testing error handling...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = robust_graph.invoke({\n",
    "    \"input\": \"What is LangGraph?\",\n",
    "    \"result\": \"\",\n",
    "    \"error\": None,\n",
    "    \"retry_count\": 0,\n",
    "    \"max_retries\": 5\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Result:\")\n",
    "print(result['result'][:200] if len(result['result']) > 200 else result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 2: Monitoring and Observability\n",
    "\n",
    "Add comprehensive logging and metrics to track graph execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitoredState(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "    metrics: dict\n",
    "\n",
    "def monitored_node(state: MonitoredState) -> MonitoredState:\n",
    "    \"\"\"Node with comprehensive monitoring.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logger.info(f\"Processing input: {state['input'][:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        response = model.invoke([HumanMessage(content=state['input'])])\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        metrics = {\n",
    "            \"execution_time\": execution_time,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"input_length\": len(state['input']),\n",
    "            \"output_length\": len(response.content),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Execution time: {execution_time:.2f}s\")\n",
    "        logger.info(f\"Output length: {len(response.content)} chars\")\n",
    "        \n",
    "        return {\n",
    "            \"output\": response.content,\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        metrics = {\n",
    "            \"execution_time\": execution_time,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "        \n",
    "        logger.error(f\"Node failed: {str(e)}\")\n",
    "        \n",
    "        return {\n",
    "            \"output\": f\"Error: {str(e)}\",\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "\n",
    "print(\"âœ“ Monitored node created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build monitored graph\n",
    "monitored_builder = StateGraph(MonitoredState)\n",
    "monitored_builder.add_node(\"process\", monitored_node)\n",
    "monitored_builder.add_edge(START, \"process\")\n",
    "monitored_builder.add_edge(\"process\", END)\n",
    "\n",
    "monitored_graph = monitored_builder.compile()\n",
    "\n",
    "# Test with monitoring\n",
    "result = monitored_graph.invoke({\n",
    "    \"input\": \"Explain LangGraph in simple terms\",\n",
    "    \"output\": \"\",\n",
    "    \"metrics\": {}\n",
    "})\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "for key, value in result['metrics'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 3: Dynamic Routing with Confidence Scores\n",
    "\n",
    "Make routing decisions based on confidence levels and quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfidenceState(TypedDict):\n",
    "    query: str\n",
    "    response: str\n",
    "    confidence: float\n",
    "    fallback_used: bool\n",
    "\n",
    "def primary_agent(state: ConfidenceState) -> ConfidenceState:\n",
    "    \"\"\"Primary agent with confidence scoring.\"\"\"\n",
    "    print(\"\\nðŸŽ¯ Primary agent processing...\")\n",
    "    \n",
    "    prompt = f\"{state['query']}\\n\\nRate your confidence in this answer from 0-1.\"\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=state['query'])])\n",
    "    \n",
    "    # Simulate confidence calculation (in practice, use actual metrics)\n",
    "    confidence = 0.85 if len(response.content) > 100 else 0.6\n",
    "    \n",
    "    print(f\"   Confidence: {confidence}\")\n",
    "    \n",
    "    return {\n",
    "        \"response\": response.content,\n",
    "        \"confidence\": confidence,\n",
    "        \"fallback_used\": False\n",
    "    }\n",
    "\n",
    "def fallback_agent(state: ConfidenceState) -> ConfidenceState:\n",
    "    \"\"\"Fallback agent for low-confidence scenarios.\"\"\"\n",
    "    print(\"\\nðŸ”„ Fallback agent activated...\")\n",
    "    \n",
    "    prompt = f\"\"\"The primary agent was uncertain. Please provide a very careful, \n",
    "    well-researched answer to: {state['query']}\"\"\"\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    return {\n",
    "        \"response\": response.content,\n",
    "        \"confidence\": 0.9,  # Fallback is more thorough\n",
    "        \"fallback_used\": True\n",
    "    }\n",
    "\n",
    "def route_by_confidence(state: ConfidenceState) -> Literal[\"fallback\", \"success\"]:\n",
    "    \"\"\"Route based on confidence threshold.\"\"\"\n",
    "    threshold = 0.75\n",
    "    \n",
    "    if state['confidence'] < threshold:\n",
    "        print(f\"âš ï¸  Low confidence ({state['confidence']}) - using fallback\")\n",
    "        return \"fallback\"\n",
    "    else:\n",
    "        print(f\"âœ… High confidence ({state['confidence']})\")\n",
    "        return \"success\"\n",
    "\n",
    "print(\"âœ“ Confidence routing nodes created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build confidence-based routing graph\n",
    "confidence_builder = StateGraph(ConfidenceState)\n",
    "\n",
    "confidence_builder.add_node(\"primary\", primary_agent)\n",
    "confidence_builder.add_node(\"fallback\", fallback_agent)\n",
    "\n",
    "confidence_builder.add_edge(START, \"primary\")\n",
    "\n",
    "confidence_builder.add_conditional_edges(\n",
    "    \"primary\",\n",
    "    route_by_confidence,\n",
    "    {\n",
    "        \"fallback\": \"fallback\",\n",
    "        \"success\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "confidence_builder.add_edge(\"fallback\", END)\n",
    "\n",
    "confidence_graph = confidence_builder.compile()\n",
    "\n",
    "print(\"âœ“ Confidence routing graph compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test confidence routing\n",
    "print(\"Test 1: Simple query (high confidence expected)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result1 = confidence_graph.invoke({\n",
    "    \"query\": \"What is Python?\",\n",
    "    \"response\": \"\",\n",
    "    \"confidence\": 0.0,\n",
    "    \"fallback_used\": False\n",
    "})\n",
    "\n",
    "print(f\"\\nUsed fallback: {result1['fallback_used']}\")\n",
    "print(f\"Final confidence: {result1['confidence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 4: Circuit Breaker Pattern\n",
    "\n",
    "Protect your system from cascading failures with circuit breakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitBreakerState(TypedDict):\n",
    "    request: str\n",
    "    response: str\n",
    "    failure_count: int\n",
    "    circuit_open: bool\n",
    "    threshold: int\n",
    "\n",
    "def protected_operation(state: CircuitBreakerState) -> CircuitBreakerState:\n",
    "    \"\"\"Operation protected by circuit breaker.\"\"\"\n",
    "    \n",
    "    if state.get('circuit_open', False):\n",
    "        logger.warning(\"Circuit breaker is OPEN - rejecting request\")\n",
    "        return {\n",
    "            \"response\": \"Service temporarily unavailable\",\n",
    "            \"circuit_open\": True\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nâš¡ Executing protected operation...\")\n",
    "        \n",
    "        # Simulate operation that might fail\n",
    "        response = model.invoke([HumanMessage(content=state['request'])])\n",
    "        \n",
    "        logger.info(\"Operation succeeded - resetting failure count\")\n",
    "        return {\n",
    "            \"response\": response.content,\n",
    "            \"failure_count\": 0,  # Reset on success\n",
    "            \"circuit_open\": False\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        new_failure_count = state.get('failure_count', 0) + 1\n",
    "        logger.error(f\"Operation failed ({new_failure_count}/{state['threshold']})\")\n",
    "        \n",
    "        # Open circuit if threshold reached\n",
    "        circuit_open = new_failure_count >= state['threshold']\n",
    "        \n",
    "        if circuit_open:\n",
    "            logger.critical(\"CIRCUIT BREAKER OPENED\")\n",
    "        \n",
    "        return {\n",
    "            \"response\": f\"Error: {str(e)}\",\n",
    "            \"failure_count\": new_failure_count,\n",
    "            \"circuit_open\": circuit_open\n",
    "        }\n",
    "\n",
    "print(\"âœ“ Circuit breaker pattern implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 5: Caching and Optimization\n",
    "\n",
    "Optimize performance with intelligent caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple in-memory cache\n",
    "response_cache = {}\n",
    "\n",
    "class CachedState(TypedDict):\n",
    "    query: str\n",
    "    response: str\n",
    "    cache_hit: bool\n",
    "\n",
    "def cached_operation(state: CachedState) -> CachedState:\n",
    "    \"\"\"Operation with caching.\"\"\"\n",
    "    query = state['query']\n",
    "    \n",
    "    # Check cache first\n",
    "    if query in response_cache:\n",
    "        print(\"\\nâœ¨ Cache HIT\")\n",
    "        logger.info(f\"Cache hit for query: {query[:50]}\")\n",
    "        return {\n",
    "            \"response\": response_cache[query],\n",
    "            \"cache_hit\": True\n",
    "        }\n",
    "    \n",
    "    # Cache miss - execute operation\n",
    "    print(\"\\nðŸ”„ Cache MISS - executing...\")\n",
    "    logger.info(f\"Cache miss for query: {query[:50]}\")\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=query)])\n",
    "    \n",
    "    # Store in cache\n",
    "    response_cache[query] = response.content\n",
    "    \n",
    "    return {\n",
    "        \"response\": response.content,\n",
    "        \"cache_hit\": False\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Caching implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cached graph\n",
    "cached_builder = StateGraph(CachedState)\n",
    "cached_builder.add_node(\"process\", cached_operation)\n",
    "cached_builder.add_edge(START, \"process\")\n",
    "cached_builder.add_edge(\"process\", END)\n",
    "\n",
    "cached_graph = cached_builder.compile()\n",
    "\n",
    "# Test caching\n",
    "print(\"Test 1: First request (cache miss)\")\n",
    "result1 = cached_graph.invoke({\n",
    "    \"query\": \"What is caching?\",\n",
    "    \"response\": \"\",\n",
    "    \"cache_hit\": False\n",
    "})\n",
    "\n",
    "print(f\"\\nCache hit: {result1['cache_hit']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test 2: Same request (cache hit)\")\n",
    "result2 = cached_graph.invoke({\n",
    "    \"query\": \"What is caching?\",\n",
    "    \"response\": \"\",\n",
    "    \"cache_hit\": False\n",
    "})\n",
    "\n",
    "print(f\"\\nCache hit: {result2['cache_hit']}\")\n",
    "print(f\"Response matches: {result1['response'] == result2['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 6: Rate Limiting\n",
    "\n",
    "Control API usage and prevent overload with rate limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, max_calls: int, time_window: int):\n",
    "        self.max_calls = max_calls\n",
    "        self.time_window = time_window\n",
    "        self.calls = deque()\n",
    "    \n",
    "    def allow_request(self) -> bool:\n",
    "        now = time.time()\n",
    "        \n",
    "        # Remove old calls outside window\n",
    "        while self.calls and self.calls[0] < now - self.time_window:\n",
    "            self.calls.popleft()\n",
    "        \n",
    "        # Check if under limit\n",
    "        if len(self.calls) < self.max_calls:\n",
    "            self.calls.append(now)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Create rate limiter: 3 calls per 10 seconds\n",
    "rate_limiter = RateLimiter(max_calls=3, time_window=10)\n",
    "\n",
    "class RateLimitedState(TypedDict):\n",
    "    request: str\n",
    "    response: str\n",
    "    rate_limited: bool\n",
    "\n",
    "def rate_limited_operation(state: RateLimitedState) -> RateLimitedState:\n",
    "    \"\"\"Operation with rate limiting.\"\"\"\n",
    "    \n",
    "    if not rate_limiter.allow_request():\n",
    "        print(\"\\nðŸš« Rate limit exceeded\")\n",
    "        logger.warning(\"Request rate limited\")\n",
    "        return {\n",
    "            \"response\": \"Rate limit exceeded. Please try again later.\",\n",
    "            \"rate_limited\": True\n",
    "        }\n",
    "    \n",
    "    print(\"\\nâœ… Request allowed\")\n",
    "    response = model.invoke([HumanMessage(content=state['request'])])\n",
    "    \n",
    "    return {\n",
    "        \"response\": response.content,\n",
    "        \"rate_limited\": False\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Rate limiting implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "### 1. Error Handling\n",
    "- Always wrap operations in try-except blocks\n",
    "- Implement retry logic with exponential backoff\n",
    "- Provide meaningful error messages\n",
    "- Have fallback strategies\n",
    "\n",
    "### 2. Monitoring\n",
    "- Log all important events\n",
    "- Track execution times\n",
    "- Monitor success/failure rates\n",
    "- Record metrics for analysis\n",
    "\n",
    "### 3. Performance\n",
    "- Cache frequently accessed data\n",
    "- Use rate limiting to protect services\n",
    "- Implement circuit breakers for resilience\n",
    "- Optimize node execution order\n",
    "\n",
    "### 4. Architecture\n",
    "- Keep nodes focused and single-purpose\n",
    "- Use clear state definitions\n",
    "- Implement proper separation of concerns\n",
    "- Make graphs testable\n",
    "\n",
    "### 5. Production Readiness\n",
    "- Add comprehensive testing\n",
    "- Implement health checks\n",
    "- Use proper configuration management\n",
    "- Document your graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build a Production-Ready Graph\n",
    "\n",
    "Combine multiple patterns to create a robust system:\n",
    "1. Add error handling with retries\n",
    "2. Implement monitoring and logging\n",
    "3. Add caching for performance\n",
    "4. Include rate limiting\n",
    "5. Use confidence-based routing\n",
    "\n",
    "Create a question-answering system with all these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "class ProductionState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    confidence: float\n",
    "    cache_hit: bool\n",
    "    retry_count: int\n",
    "    error: Optional[str]\n",
    "    metrics: dict\n",
    "\n",
    "# TODO: Implement nodes that combine:\n",
    "# - Caching\n",
    "# - Rate limiting\n",
    "# - Error handling with retries\n",
    "# - Monitoring\n",
    "# - Confidence-based routing\n",
    "\n",
    "# TODO: Build the graph\n",
    "# TODO: Test with various scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. âœ… Implementing robust error handling and recovery\n",
    "2. âœ… Adding comprehensive monitoring and logging\n",
    "3. âœ… Using confidence scores for intelligent routing\n",
    "4. âœ… Implementing circuit breaker patterns\n",
    "5. âœ… Optimizing with caching strategies\n",
    "6. âœ… Adding rate limiting for protection\n",
    "7. âœ… Following production-ready best practices\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the LangGraph training! You now have the knowledge to build:\n",
    "- Simple chatbots\n",
    "- Complex multi-agent systems\n",
    "- Production-ready workflows\n",
    "- Robust, scalable applications\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Build your own LangGraph application\n",
    "- Explore the [official documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- Join the community and share your projects\n",
    "- Keep learning and experimenting!\n",
    "\n",
    "Happy building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
